{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP6Vi8GaMulNIFAL3sDNq2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalagrawal2/semantic-entropy-probes/blob/main/generate_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install conda\n",
        "conda config --set channel_priority strict\n",
        "conda env update -f sep_enviroment.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "JGfxx_prRqnT",
        "outputId": "b78a38ff-dca1-40f1-ff88-2ad6add72641"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Ignored the following yanked versions: 3.0.6, 3.5.0, 3.7.0, 3.17.0, 4.0.0, 4.0.1, 4.0.2, 4.0.3, 4.0.4, 4.0.5, 4.0.7, 4.0.8, 4.0.9, 4.1.2, 4.1.6, 4.2.6, 4.2.7, 4.3.13, 4.3.16\n",
            "ERROR: Could not find a version that satisfies the requirement conda (from versions: none)\n",
            "ERROR: No matching distribution found for conda\n",
            "bash: line 2: conda: command not found\n",
            "bash: line 3: conda: command not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'pip install conda\\nconda config --set channel_priority strict\\nconda env update -f sep_enviroment.yaml \\n'' returned non-zero exit status 127.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c33a08fc1b26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pip install conda\\nconda config --set channel_priority strict\\nconda env update -f sep_enviroment.yaml \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'pip install conda\\nconda config --set channel_priority strict\\nconda env update -f sep_enviroment.yaml \\n'' returned non-zero exit status 127."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "TXkmkqGHG9H2",
        "outputId": "847a3248-c8d9-4f28-dfaa-e1e3af1639b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'uncertainty'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-19edb5984a27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muncertainty_measures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mp_true\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp_true_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'uncertainty'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\"\"\"Predict with LLM on task.\"\"\"\n",
        "import gc\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import openai\n",
        "import wandb\n",
        "\n",
        "from uncertainty.data.data_utils import load_ds\n",
        "from uncertainty.utils import utils\n",
        "from uncertainty.uncertainty_measures import p_true as p_true_utils\n",
        "from compute_uncertainty_measures import main as main_compute\n",
        "\n",
        "\n",
        "utils.setup_logger()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Set up OpenAI API credentials.\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    if args.dataset == 'svamp':\n",
        "        if not args.use_context:\n",
        "            logging.info('Forcing `use_context=True` for svamp dataset.')\n",
        "            args.use_context = True\n",
        "    elif args.dataset == 'squad':\n",
        "        if not args.answerable_only:\n",
        "            logging.info('Forcing `answerable_only=True` for squad dataset.')\n",
        "            args.answerable_only = True\n",
        "\n",
        "    experiment_details = {'args': args}\n",
        "    random.seed(args.random_seed)\n",
        "\n",
        "    # Implement\n",
        "    user = os.environ['USER']\n",
        "    entity = os.environ['WANDB_ENT']\n",
        "    slurm_jobid = os.getenv('SLURM_JOB_ID', None)\n",
        "    scratch_dir = os.getenv('SCRATCH_DIR', '.')\n",
        "    if not os.path.exists(f\"{scratch_dir}/{user}/uncertainty\"):\n",
        "        os.makedirs(f\"{scratch_dir}/{user}/uncertainty\")\n",
        "\n",
        "    wandb.init(\n",
        "        entity=entity,\n",
        "        project=\"semantic_uncertainty\" if not args.debug else \"semantic_uncertainty_debug\",\n",
        "        dir=f\"{scratch_dir}/{user}/uncertainty\",\n",
        "        config=args,\n",
        "        notes=f'slurm_id: {slurm_jobid}, experiment_lot: {args.experiment_lot}',\n",
        "    )\n",
        "    logging.info('Finished wandb init.')\n",
        "\n",
        "    metric = utils.get_metric(args.metric)\n",
        "\n",
        "    train_dataset, validation_dataset = load_ds(\n",
        "        args.dataset, add_options=args.use_mc_options, seed=args.random_seed)\n",
        "    if args.ood_train_dataset is not None:\n",
        "        logging.warning(\n",
        "            'Using OOD dataset %s to construct few-shot prompts and train p_ik.',\n",
        "            args.ood_train_dataset)\n",
        "        # Get indices of answerable and unanswerable questions and construct prompt.\n",
        "        train_dataset, _ = load_ds(args.ood_train_dataset, add_options=args.use_mc_options)\n",
        "    if not isinstance(train_dataset, list):\n",
        "        logging.info('Train dataset: %s', train_dataset)\n",
        "\n",
        "    # Get indices of answerable and unanswerable questions and construct prompt.\n",
        "    answerable_indices, unanswerable_indices = utils.split_dataset(train_dataset)\n",
        "\n",
        "    if args.answerable_only:\n",
        "        unanswerable_indices = []\n",
        "        val_answerable, val_unanswerable = utils.split_dataset(validation_dataset)\n",
        "        del val_unanswerable\n",
        "        validation_dataset = [validation_dataset[i] for i in val_answerable]\n",
        "\n",
        "    prompt_indices = random.sample(answerable_indices, args.num_few_shot)\n",
        "    experiment_details['prompt_indices'] = prompt_indices\n",
        "    remaining_answerable = list(set(answerable_indices) - set(prompt_indices))\n",
        "\n",
        "    # Create Few-Shot prompt.\n",
        "    make_prompt = utils.get_make_prompt(args)\n",
        "    BRIEF = utils.BRIEF_PROMPTS[args.brief_prompt]\n",
        "    arg = args.brief_always if args.enable_brief else True\n",
        "    prompt = utils.construct_fewshot_prompt_from_indices(\n",
        "        train_dataset, prompt_indices, BRIEF, arg, make_prompt)\n",
        "    experiment_details['prompt'] = prompt\n",
        "    experiment_details['BRIEF'] = BRIEF\n",
        "    logging.info('Prompt is: %s', prompt)\n",
        "\n",
        "    # Initialize model.\n",
        "    model = utils.init_model(args)\n",
        "\n",
        "    # Initialize prompt for p_true baseline.\n",
        "    if args.compute_p_true:\n",
        "        logging.info(80*'#')\n",
        "        logging.info('Constructing few-shot prompt for p_true.')\n",
        "\n",
        "        p_true_indices = random.sample(answerable_indices, args.p_true_num_fewshot)\n",
        "        remaining_answerable = list(set(remaining_answerable) - set(p_true_indices))\n",
        "        p_true_few_shot_prompt, p_true_responses, len_p_true = p_true_utils.construct_few_shot_prompt(\n",
        "            model=model, dataset=train_dataset, indices=p_true_indices,\n",
        "            prompt=prompt, brief=BRIEF,\n",
        "            brief_always=args.brief_always and args.enable_brief,\n",
        "            make_prompt=make_prompt, num_generations=args.num_generations,\n",
        "            metric=metric)\n",
        "        wandb.config.update(\n",
        "            {'p_true_num_fewshot': len_p_true}, allow_val_change=True)\n",
        "        wandb.log(dict(len_p_true=len_p_true))\n",
        "        experiment_details['p_true_indices'] = p_true_indices\n",
        "        experiment_details['p_true_responses'] = p_true_responses\n",
        "        experiment_details['p_true_few_shot_prompt'] = p_true_few_shot_prompt\n",
        "        logging.info('Finished constructing few-shot prompt for p_true.')\n",
        "        logging.info(80*'#')\n",
        "        logging.info('p_true_few_shot_prompt: %s', p_true_few_shot_prompt)\n",
        "        logging.info(80*'#')\n",
        "\n",
        "    # Start answer generation.\n",
        "    logging.info(80 * '=')\n",
        "    logging.info('Generating answers: ')\n",
        "    logging.info(80 * '=')\n",
        "    for dataset_split in ['train', 'validation']:\n",
        "        logging.info(80 * 'x')\n",
        "        logging.info('Starting with dataset_split %s.', dataset_split)\n",
        "        logging.info(80 * 'x')\n",
        "\n",
        "        # This will store all input data and model predictions.\n",
        "        accuracies, generations, results_dict, p_trues = [], {}, {}, []\n",
        "\n",
        "        if dataset_split == 'train':\n",
        "            if not args.get_training_set_generations:\n",
        "                logging.info('Skip training data.')\n",
        "                continue\n",
        "            dataset = train_dataset\n",
        "            possible_indices = list(set(remaining_answerable) | set(unanswerable_indices))\n",
        "\n",
        "        else:\n",
        "            dataset = validation_dataset\n",
        "            possible_indices = range(0, len(dataset))\n",
        "\n",
        "        # Evaluate over random subset of the datasets.\n",
        "        indices = random.sample(possible_indices, min(args.num_samples, len(dataset)))\n",
        "        experiment_details[dataset_split] = {'indices': indices}\n",
        "\n",
        "        if args.num_samples > len(dataset):\n",
        "            logging.warning('Not enough samples in dataset. Using all %d samples.', len(dataset))\n",
        "\n",
        "        it = 0\n",
        "        for index in tqdm(indices):\n",
        "            if (it + 1 % 10) == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "            it += 1\n",
        "\n",
        "            # Grab example at index.\n",
        "            example = dataset[index]\n",
        "            question, context = example[\"question\"], example['context']\n",
        "            generations[example['id']] = {'question': question, 'context': context}\n",
        "            correct_answer = example['answers']['text']\n",
        "\n",
        "            current_input = make_prompt(\n",
        "                context, question, None, BRIEF, args.brief_always and args.enable_brief)\n",
        "            local_prompt = prompt + current_input\n",
        "\n",
        "            logging.info('Current input: '.ljust(15) + current_input)\n",
        "\n",
        "            full_responses = []\n",
        "\n",
        "            # We sample 1 low temperature answer on which we will compute the\n",
        "            # accuracy and args.num_generation high temperature answers which will\n",
        "            # be used to estimate the entropy.\n",
        "\n",
        "            if dataset_split == 'train' and args.get_training_set_generations_most_likely_only:\n",
        "                num_generations = 1\n",
        "            else:\n",
        "                num_generations = args.num_generations + 1\n",
        "\n",
        "            for i in range(num_generations):\n",
        "\n",
        "                # Temperature for first generation is always `0.1`.\n",
        "                temperature = 0.1 if i == 0 else args.temperature\n",
        "\n",
        "                predicted_answer, token_log_likelihoods, (embedding, emb_last_before_gen, emb_before_eos) = model.predict(local_prompt, temperature, return_latent=True)\n",
        "\n",
        "                # Last token embedding\n",
        "                embedding = embedding.cpu() if embedding is not None else None\n",
        "                emb_last_before_gen = emb_last_before_gen.cpu() if emb_last_before_gen is not None else None\n",
        "                emb_before_eos = emb_before_eos.cpu() if emb_before_eos is not None else None\n",
        "\n",
        "                compute_acc = args.compute_accuracy_at_all_temps or (i == 0)\n",
        "                if correct_answer and compute_acc:\n",
        "                    acc = metric(predicted_answer, example, model)\n",
        "                else:\n",
        "                    acc = 0.0  # pylint: disable=invalid-name\n",
        "\n",
        "                if i == 0:\n",
        "                    # Logging.\n",
        "                    logging.info('Iteration ' + str(it) + ':  ' + 80*'#')\n",
        "                    if args.use_context:\n",
        "                        logging.info('context: '.ljust(15) + str(context))\n",
        "                    logging.info('question: '.ljust(15) + question)\n",
        "                    logging.info('low-t prediction: '.ljust(15) + predicted_answer)\n",
        "                    logging.info('correct answer: '.ljust(15) + str(correct_answer))\n",
        "                    logging.info('accuracy: '.ljust(15) + str(acc))\n",
        "\n",
        "                    accuracies.append(acc)\n",
        "                    most_likely_answer_dict = {\n",
        "                        'response': predicted_answer,\n",
        "                        'token_log_likelihoods': token_log_likelihoods,\n",
        "                        'embedding': embedding,\n",
        "                        'accuracy': acc,\n",
        "                        'emb_last_tok_before_gen': emb_last_before_gen,\n",
        "                        'emb_tok_before_eos': emb_before_eos,\n",
        "                    }\n",
        "\n",
        "                    generations[example['id']].update({\n",
        "                        'most_likely_answer': most_likely_answer_dict,\n",
        "                        'reference': utils.get_reference(example),\n",
        "                    })\n",
        "                else:\n",
        "                    logging.info('high-t prediction '.ljust(15) + str(i) + ' : ' + predicted_answer)\n",
        "                    # Aggregate predictions over num_generations.\n",
        "                    full_responses.append(\n",
        "                        (predicted_answer, token_log_likelihoods, embedding, acc))\n",
        "\n",
        "            # Append all predictions for this example to `generations`.\n",
        "            generations[example['id']]['responses'] = full_responses\n",
        "\n",
        "            if args.compute_p_true and dataset_split == 'validation':\n",
        "                # Already compute p_true here. Avoid cost of generations in compute_uncertainty script.\n",
        "                p_true = p_true_utils.calculate_p_true(\n",
        "                    model, question, most_likely_answer_dict['response'],\n",
        "                    [r[0] for r in full_responses], p_true_few_shot_prompt,\n",
        "                    hint=args.p_true_hint)\n",
        "                p_trues.append(p_true)\n",
        "                logging.info('p_true: %s', p_true)\n",
        "\n",
        "        # Save generations for that split.\n",
        "        utils.save(generations, f'{dataset_split}_generations.pkl')\n",
        "\n",
        "        # Log overall accuracy.\n",
        "        accuracy = np.mean(accuracies)\n",
        "        print(f\"Overall {dataset_split} split accuracy: {accuracy}\")\n",
        "        wandb.log({f\"{dataset_split}_accuracy\": accuracy})\n",
        "\n",
        "        if dataset_split == 'validation':\n",
        "            if args.compute_p_true:\n",
        "                results_dict['uncertainty_measures'] = {\n",
        "                    'p_false':  [1 - p for p in p_trues],\n",
        "                    'p_false_fixed':  [1 - np.exp(p) for p in p_trues],\n",
        "                }\n",
        "            utils.save(results_dict, 'uncertainty_measures.pkl')\n",
        "\n",
        "    utils.save(experiment_details, 'experiment_details.pkl')\n",
        "    logging.info('Run complete.')\n",
        "    del model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = utils.get_parser()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    logging.info('Starting new run with args: %s', args)\n",
        "\n",
        "    if unknown:\n",
        "        raise ValueError(f'Unkown args: {unknown}')\n",
        "\n",
        "    if args.compute_uncertainties:\n",
        "        args.assign_new_wandb_id = False\n",
        "\n",
        "    logging.info('STARTING `generate_answers`!')\n",
        "    main(args)\n",
        "    logging.info('FINISHED `generate_answers`!')\n",
        "\n",
        "    if args.compute_uncertainties:\n",
        "        logging.info(50 * '#X')\n",
        "        logging.info('STARTING `compute_uncertainty_measures`!')\n",
        "        main_compute(args)\n",
        "        logging.info('FINISHED `compute_uncertainty_measures`!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python generate_answers.py --model_name=Mistral-7B-v0.1 --dataset=trivia_qa"
      ],
      "metadata": {
        "id": "pbanEJkgHx28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}