{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOnYwVQac4nUrz5AiCUquRo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalagrawal2/semantic-entropy-probes/blob/main/generate_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "JGfxx_prRqnT",
        "outputId": "c76ca156-f62b-458e-d77a-b4314db5a1fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: pip: command not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'condacolab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dfa229587fce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q condacolab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'condacolab'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "condacolab.check()"
      ],
      "metadata": {
        "id": "2uCEG05VE-EV",
        "outputId": "3cad5329-42a0-46fb-9b29-cb0df49c8745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'condacolab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9ce5e157f4c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'condacolab'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #need to add enviroment file to drive before running this cell\n",
        " !conda env update -f sep_enviroment.yaml"
      ],
      "metadata": {
        "id": "W2uHpI5fFAXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing dependencies\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install uncertainty"
      ],
      "metadata": {
        "id": "_E8XDo6xIC2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXkmkqGHG9H2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Predict with LLM on task.\"\"\"\n",
        "import gc\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import openai\n",
        "import wandb\n",
        "\n",
        "from uncertainty.data.data_utils import load_ds\n",
        "from uncertainty.utils import utils\n",
        "from uncertainty.uncertainty_measures import p_true as p_true_utils\n",
        "from compute_uncertainty_measures import main as main_compute\n",
        "\n",
        "\n",
        "utils.setup_logger()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Set up OpenAI API credentials.\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    if args.dataset == 'svamp':\n",
        "        if not args.use_context:\n",
        "            logging.info('Forcing `use_context=True` for svamp dataset.')\n",
        "            args.use_context = True\n",
        "    elif args.dataset == 'squad':\n",
        "        if not args.answerable_only:\n",
        "            logging.info('Forcing `answerable_only=True` for squad dataset.')\n",
        "            args.answerable_only = True\n",
        "\n",
        "    experiment_details = {'args': args}\n",
        "    random.seed(args.random_seed)\n",
        "\n",
        "    # Implement\n",
        "    user = os.environ['USER']\n",
        "    entity = os.environ['WANDB_ENT']\n",
        "    slurm_jobid = os.getenv('SLURM_JOB_ID', None)\n",
        "    scratch_dir = os.getenv('SCRATCH_DIR', '.')\n",
        "    if not os.path.exists(f\"{scratch_dir}/{user}/uncertainty\"):\n",
        "        os.makedirs(f\"{scratch_dir}/{user}/uncertainty\")\n",
        "\n",
        "    wandb.init(\n",
        "        entity=entity,\n",
        "        project=\"semantic_uncertainty\" if not args.debug else \"semantic_uncertainty_debug\",\n",
        "        dir=f\"{scratch_dir}/{user}/uncertainty\",\n",
        "        config=args,\n",
        "        notes=f'slurm_id: {slurm_jobid}, experiment_lot: {args.experiment_lot}',\n",
        "    )\n",
        "    logging.info('Finished wandb init.')\n",
        "\n",
        "    metric = utils.get_metric(args.metric)\n",
        "\n",
        "    train_dataset, validation_dataset = load_ds(\n",
        "        args.dataset, add_options=args.use_mc_options, seed=args.random_seed)\n",
        "    if args.ood_train_dataset is not None:\n",
        "        logging.warning(\n",
        "            'Using OOD dataset %s to construct few-shot prompts and train p_ik.',\n",
        "            args.ood_train_dataset)\n",
        "        # Get indices of answerable and unanswerable questions and construct prompt.\n",
        "        train_dataset, _ = load_ds(args.ood_train_dataset, add_options=args.use_mc_options)\n",
        "    if not isinstance(train_dataset, list):\n",
        "        logging.info('Train dataset: %s', train_dataset)\n",
        "\n",
        "    # Get indices of answerable and unanswerable questions and construct prompt.\n",
        "    answerable_indices, unanswerable_indices = utils.split_dataset(train_dataset)\n",
        "\n",
        "    if args.answerable_only:\n",
        "        unanswerable_indices = []\n",
        "        val_answerable, val_unanswerable = utils.split_dataset(validation_dataset)\n",
        "        del val_unanswerable\n",
        "        validation_dataset = [validation_dataset[i] for i in val_answerable]\n",
        "\n",
        "    prompt_indices = random.sample(answerable_indices, args.num_few_shot)\n",
        "    experiment_details['prompt_indices'] = prompt_indices\n",
        "    remaining_answerable = list(set(answerable_indices) - set(prompt_indices))\n",
        "\n",
        "    # Create Few-Shot prompt.\n",
        "    make_prompt = utils.get_make_prompt(args)\n",
        "    BRIEF = utils.BRIEF_PROMPTS[args.brief_prompt]\n",
        "    arg = args.brief_always if args.enable_brief else True\n",
        "    prompt = utils.construct_fewshot_prompt_from_indices(\n",
        "        train_dataset, prompt_indices, BRIEF, arg, make_prompt)\n",
        "    experiment_details['prompt'] = prompt\n",
        "    experiment_details['BRIEF'] = BRIEF\n",
        "    logging.info('Prompt is: %s', prompt)\n",
        "\n",
        "    # Initialize model.\n",
        "    model = utils.init_model(args)\n",
        "\n",
        "    # Initialize prompt for p_true baseline.\n",
        "    if args.compute_p_true:\n",
        "        logging.info(80*'#')\n",
        "        logging.info('Constructing few-shot prompt for p_true.')\n",
        "\n",
        "        p_true_indices = random.sample(answerable_indices, args.p_true_num_fewshot)\n",
        "        remaining_answerable = list(set(remaining_answerable) - set(p_true_indices))\n",
        "        p_true_few_shot_prompt, p_true_responses, len_p_true = p_true_utils.construct_few_shot_prompt(\n",
        "            model=model, dataset=train_dataset, indices=p_true_indices,\n",
        "            prompt=prompt, brief=BRIEF,\n",
        "            brief_always=args.brief_always and args.enable_brief,\n",
        "            make_prompt=make_prompt, num_generations=args.num_generations,\n",
        "            metric=metric)\n",
        "        wandb.config.update(\n",
        "            {'p_true_num_fewshot': len_p_true}, allow_val_change=True)\n",
        "        wandb.log(dict(len_p_true=len_p_true))\n",
        "        experiment_details['p_true_indices'] = p_true_indices\n",
        "        experiment_details['p_true_responses'] = p_true_responses\n",
        "        experiment_details['p_true_few_shot_prompt'] = p_true_few_shot_prompt\n",
        "        logging.info('Finished constructing few-shot prompt for p_true.')\n",
        "        logging.info(80*'#')\n",
        "        logging.info('p_true_few_shot_prompt: %s', p_true_few_shot_prompt)\n",
        "        logging.info(80*'#')\n",
        "\n",
        "    # Start answer generation.\n",
        "    logging.info(80 * '=')\n",
        "    logging.info('Generating answers: ')\n",
        "    logging.info(80 * '=')\n",
        "    for dataset_split in ['train', 'validation']:\n",
        "        logging.info(80 * 'x')\n",
        "        logging.info('Starting with dataset_split %s.', dataset_split)\n",
        "        logging.info(80 * 'x')\n",
        "\n",
        "        # This will store all input data and model predictions.\n",
        "        accuracies, generations, results_dict, p_trues = [], {}, {}, []\n",
        "\n",
        "        if dataset_split == 'train':\n",
        "            if not args.get_training_set_generations:\n",
        "                logging.info('Skip training data.')\n",
        "                continue\n",
        "            dataset = train_dataset\n",
        "            possible_indices = list(set(remaining_answerable) | set(unanswerable_indices))\n",
        "\n",
        "        else:\n",
        "            dataset = validation_dataset\n",
        "            possible_indices = range(0, len(dataset))\n",
        "\n",
        "        # Evaluate over random subset of the datasets.\n",
        "        indices = random.sample(possible_indices, min(args.num_samples, len(dataset)))\n",
        "        experiment_details[dataset_split] = {'indices': indices}\n",
        "\n",
        "        if args.num_samples > len(dataset):\n",
        "            logging.warning('Not enough samples in dataset. Using all %d samples.', len(dataset))\n",
        "\n",
        "        it = 0\n",
        "        for index in tqdm(indices):\n",
        "            if (it + 1 % 10) == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "            it += 1\n",
        "\n",
        "            # Grab example at index.\n",
        "            example = dataset[index]\n",
        "            question, context = example[\"question\"], example['context']\n",
        "            generations[example['id']] = {'question': question, 'context': context}\n",
        "            correct_answer = example['answers']['text']\n",
        "\n",
        "            current_input = make_prompt(\n",
        "                context, question, None, BRIEF, args.brief_always and args.enable_brief)\n",
        "            local_prompt = prompt + current_input\n",
        "\n",
        "            logging.info('Current input: '.ljust(15) + current_input)\n",
        "\n",
        "            full_responses = []\n",
        "\n",
        "            # We sample 1 low temperature answer on which we will compute the\n",
        "            # accuracy and args.num_generation high temperature answers which will\n",
        "            # be used to estimate the entropy.\n",
        "\n",
        "            if dataset_split == 'train' and args.get_training_set_generations_most_likely_only:\n",
        "                num_generations = 1\n",
        "            else:\n",
        "                num_generations = args.num_generations + 1\n",
        "\n",
        "            for i in range(num_generations):\n",
        "\n",
        "                # Temperature for first generation is always `0.1`.\n",
        "                temperature = 0.1 if i == 0 else args.temperature\n",
        "\n",
        "                predicted_answer, token_log_likelihoods, (embedding, emb_last_before_gen, emb_before_eos) = model.predict(local_prompt, temperature, return_latent=True)\n",
        "\n",
        "                # Last token embedding\n",
        "                embedding = embedding.cpu() if embedding is not None else None\n",
        "                emb_last_before_gen = emb_last_before_gen.cpu() if emb_last_before_gen is not None else None\n",
        "                emb_before_eos = emb_before_eos.cpu() if emb_before_eos is not None else None\n",
        "\n",
        "                compute_acc = args.compute_accuracy_at_all_temps or (i == 0)\n",
        "                if correct_answer and compute_acc:\n",
        "                    acc = metric(predicted_answer, example, model)\n",
        "                else:\n",
        "                    acc = 0.0  # pylint: disable=invalid-name\n",
        "\n",
        "                if i == 0:\n",
        "                    # Logging.\n",
        "                    logging.info('Iteration ' + str(it) + ':  ' + 80*'#')\n",
        "                    if args.use_context:\n",
        "                        logging.info('context: '.ljust(15) + str(context))\n",
        "                    logging.info('question: '.ljust(15) + question)\n",
        "                    logging.info('low-t prediction: '.ljust(15) + predicted_answer)\n",
        "                    logging.info('correct answer: '.ljust(15) + str(correct_answer))\n",
        "                    logging.info('accuracy: '.ljust(15) + str(acc))\n",
        "\n",
        "                    accuracies.append(acc)\n",
        "                    most_likely_answer_dict = {\n",
        "                        'response': predicted_answer,\n",
        "                        'token_log_likelihoods': token_log_likelihoods,\n",
        "                        'embedding': embedding,\n",
        "                        'accuracy': acc,\n",
        "                        'emb_last_tok_before_gen': emb_last_before_gen,\n",
        "                        'emb_tok_before_eos': emb_before_eos,\n",
        "                    }\n",
        "\n",
        "                    generations[example['id']].update({\n",
        "                        'most_likely_answer': most_likely_answer_dict,\n",
        "                        'reference': utils.get_reference(example),\n",
        "                    })\n",
        "                else:\n",
        "                    logging.info('high-t prediction '.ljust(15) + str(i) + ' : ' + predicted_answer)\n",
        "                    # Aggregate predictions over num_generations.\n",
        "                    full_responses.append(\n",
        "                        (predicted_answer, token_log_likelihoods, embedding, acc))\n",
        "\n",
        "            # Append all predictions for this example to `generations`.\n",
        "            generations[example['id']]['responses'] = full_responses\n",
        "\n",
        "            if args.compute_p_true and dataset_split == 'validation':\n",
        "                # Already compute p_true here. Avoid cost of generations in compute_uncertainty script.\n",
        "                p_true = p_true_utils.calculate_p_true(\n",
        "                    model, question, most_likely_answer_dict['response'],\n",
        "                    [r[0] for r in full_responses], p_true_few_shot_prompt,\n",
        "                    hint=args.p_true_hint)\n",
        "                p_trues.append(p_true)\n",
        "                logging.info('p_true: %s', p_true)\n",
        "\n",
        "        # Save generations for that split.\n",
        "        utils.save(generations, f'{dataset_split}_generations.pkl')\n",
        "\n",
        "        # Log overall accuracy.\n",
        "        accuracy = np.mean(accuracies)\n",
        "        print(f\"Overall {dataset_split} split accuracy: {accuracy}\")\n",
        "        wandb.log({f\"{dataset_split}_accuracy\": accuracy})\n",
        "\n",
        "        if dataset_split == 'validation':\n",
        "            if args.compute_p_true:\n",
        "                results_dict['uncertainty_measures'] = {\n",
        "                    'p_false':  [1 - p for p in p_trues],\n",
        "                    'p_false_fixed':  [1 - np.exp(p) for p in p_trues],\n",
        "                }\n",
        "            utils.save(results_dict, 'uncertainty_measures.pkl')\n",
        "\n",
        "    utils.save(experiment_details, 'experiment_details.pkl')\n",
        "    logging.info('Run complete.')\n",
        "    del model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = utils.get_parser()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    logging.info('Starting new run with args: %s', args)\n",
        "\n",
        "    if unknown:\n",
        "        raise ValueError(f'Unkown args: {unknown}')\n",
        "\n",
        "    if args.compute_uncertainties:\n",
        "        args.assign_new_wandb_id = False\n",
        "\n",
        "    logging.info('STARTING `generate_answers`!')\n",
        "    main(args)\n",
        "    logging.info('FINISHED `generate_answers`!')\n",
        "\n",
        "    if args.compute_uncertainties:\n",
        "        logging.info(50 * '#X')\n",
        "        logging.info('STARTING `compute_uncertainty_measures`!')\n",
        "        main_compute(args)\n",
        "        logging.info('FINISHED `compute_uncertainty_measures`!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python generate_answers.py --model_name=Mistral-7B-v0.1 --dataset=trivia_qa"
      ],
      "metadata": {
        "id": "pbanEJkgHx28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}