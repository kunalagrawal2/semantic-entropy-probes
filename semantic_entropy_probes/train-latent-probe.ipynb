{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a19ef980-9f45-48f6-95c3-6cd3a60b85c7",
      "metadata": {
        "id": "a19ef980-9f45-48f6-95c3-6cd3a60b85c7"
      },
      "source": [
        "# Semantic Entropy Probes Training\n",
        "\n",
        "Coding linear probes is easy - run logistic regression on saved hidden states from SE generations, and predict binarized SE labels and model correctness. We will replicate our experimental results in this notebook, and we share our example runs for Llama-2-7B short-form generations at [this project](https://wandb.ai/jiatongg/public_semantic_uncertainty) if you want a quick start. We keep some of our visualizations (for Llama2-7b, short-form generations, {NQ Open, TriviaQA, SQuAD, BioASQ}) for quick references.\n",
        "\n",
        "For clarity, there are some variable naming conventions in this notebooks: `a` means accuracy, `b` means binarized entropy, `s` means SLT token, `t` means TBG token, `i` means In-Distribution (ID), `o` means Out-Of-Distribution (OOD).\n",
        "\n",
        "\n",
        "This notebook is arranged in sections:\n",
        "1) [Imports and Downloads](#Imports-and-Downloads) helps you load `wandb` runs into local storage;\n",
        "2) [Data Preparation](#Data-Preparation) section prepares the training data, encapsulates the training and evaluation codes, and contains some visualization tools;\n",
        "3) [Probing Acc/SE from Hidden States (id)](#Probing-Acc/SE-from-Hidden-States) section binarizes SE and carries out actual training of SEPs and Acc. Pr. in the In-Distribution setting, where we train and test on the same dataset yet on different splits;\n",
        "4) [Test probes trained with one dataset on others](#Test-probes-trained-with-one-dataset-on-others) section tests SEPs and Acc. Pr. performances in predicting model correctness on other datasets;\n",
        "5) The rest sections ([5](#Baselines-and-Performance-Plots), [6](#Save-trained-probes-for-inference)) are for performance comparisons with baselines and model saving."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7b6d7a-664a-40d3-8353-24bd49dd3d1f",
      "metadata": {
        "id": "1f7b6d7a-664a-40d3-8353-24bd49dd3d1f"
      },
      "source": [
        "## Imports and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "63d475c9-8833-46ed-9387-f2e399fad23f",
      "metadata": {
        "scrolled": true,
        "id": "63d475c9-8833-46ed-9387-f2e399fad23f"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import torch\n",
        "import wandb\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "rng = np.random.default_rng(42)\n",
        "run_files = {\n",
        "    'UNC_MEA': 'uncertainty_measures.pkl',\n",
        "    'VAL_GEN': 'validation_generations.pkl', # essential for SEP training\n",
        "    'WAN_SUM': 'wandb-summary.json'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c383552-9fef-439c-9498-dcdc00c70214",
      "metadata": {
        "id": "7c383552-9fef-439c-9498-dcdc00c70214"
      },
      "outputs": [],
      "source": [
        "# TO FILL\n",
        "model_name = 'Llama2-7b' # replace with your model name\n",
        "wandb_run_ids = ['jiatongg/public_semantic_uncertainty/mb6th09c',\n",
        "                 'jiatongg/public_semantic_uncertainty/x84t13ca',\n",
        "                 'jiatongg/public_semantic_uncertainty/wxvq5m6a',\n",
        "                 'jiatongg/public_semantic_uncertainty/9qv44yea']\n",
        "                # example ids runs; format: entity/project/run_id; replace with your wandb run ids\n",
        "ds_names = ['bioasq', 'trivia-qa', 'nq', 'squad'] # replace with your actual dataset names (corresponding to run ids)\n",
        "notebook_path = '~/semantic_entropy_probes' # replace with your notebook path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(notebook_path, exist_ok=True)\n",
        "os.chdir(notebook_path)"
      ],
      "metadata": {
        "id": "KJ1mNRzuhkyv"
      },
      "id": "KJ1mNRzuhkyv",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f57f1fa6-4cb9-406e-b4d3-767a9bedac7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57f1fa6-4cb9-406e-b4d3-767a9bedac7f",
        "outputId": "0ed6f495-98e4-4791-d47d-1b3d6c021f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkunalagrawal\u001b[0m (\u001b[33mkunalagrawal-university-of-california-berkeley\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "# ensure you are logged in before downloads\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "92ee5160-f157-4329-a812-092e8f47dbb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ee5160-f157-4329-a812-092e8f47dbb1",
        "outputId": "cfd1343e-cf58-47d4-f0eb-bfc252c474df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uncertainty_measures.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-mb6th09c\n",
            "validation_generations.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-mb6th09c\n",
            "wandb-summary.json downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-mb6th09c\n",
            "uncertainty_measures.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-x84t13ca\n",
            "validation_generations.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-x84t13ca\n",
            "wandb-summary.json downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-x84t13ca\n",
            "uncertainty_measures.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-wxvq5m6a\n",
            "validation_generations.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-wxvq5m6a\n",
            "wandb-summary.json downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-wxvq5m6a\n",
            "uncertainty_measures.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-9qv44yea\n",
            "validation_generations.pkl downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-9qv44yea\n",
            "wandb-summary.json downloaded to path content/semantic_entropy_probes/wandb_runs/Llama2-7b/run-9qv44yea\n"
          ]
        }
      ],
      "source": [
        "# Download from wandb\n",
        "paths_to_save_runs = [f\"wandb_runs/{model_name}/run-{run_id.split('/')[-1]}\" for run_id in wandb_run_ids]\n",
        "for path, run_id in zip(paths_to_save_runs, wandb_run_ids):\n",
        "    full_path = os.path.expanduser(path)\n",
        "    full_path = os.path.join(notebook_path, full_path)\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "    run = wandb.Api().run(run_id)\n",
        "    for file_name in run_files.values():\n",
        "        file = run.file(file_name)\n",
        "        file.download(full_path, exist_ok=False, replace=True)\n",
        "        print(f'{file_name} downloaded to path {full_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c329c604-1dbf-4a3b-9104-95646803718f",
      "metadata": {
        "id": "c329c604-1dbf-4a3b-9104-95646803718f"
      },
      "outputs": [],
      "source": [
        "# Paths need to contain 'validation_generations.pkl' and 'uncertainty_measures.pkl'\n",
        "ds_paths = paths_to_save_runs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(os.path.join(notebook_path, ds_paths[0]))"
      ],
      "metadata": {
        "id": "diHqSPNzk_ja",
        "outputId": "75628814-9b57-4174-9e08-634d6b628a8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "diHqSPNzk_ja",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['validation_generations.pkl',\n",
              " 'uncertainty_measures.pkl',\n",
              " 'wandb-summary.json']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1f2d66-6ec4-4aa8-9226-64921689fe9e",
      "metadata": {
        "id": "9c1f2d66-6ec4-4aa8-9226-64921689fe9e"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "675c3a21-536a-4ee9-9f12-0b4da58c20a3",
      "metadata": {
        "id": "675c3a21-536a-4ee9-9f12-0b4da58c20a3"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for easier attribute keeping\n",
        "class Dataset:\n",
        "    def __init__(self, values):\n",
        "        self.tbg_dataset = values[0]\n",
        "        self.slt_dataset = values[1]\n",
        "        self.entropy = values[2]\n",
        "        self.accuracies = values[3]\n",
        "\n",
        "def load_dataset(path, n_sample=2000):\n",
        "\n",
        "    os.makedirs(os.path.join(notebook_path, path), exist_ok=True)\n",
        "    os.chdir(os.path.join(notebook_path, path))\n",
        "\n",
        "    # Read validation generated embeddings\n",
        "    with open(run_files['VAL_GEN'], 'rb') as f:\n",
        "        generations = pickle.load(f)\n",
        "\n",
        "    # Read uncertainty measures (p-true, predictive/semantic uncertainties)\n",
        "    with open(run_files['UNC_MEA'], 'rb') as g:\n",
        "        measures = pickle.load(g)\n",
        "\n",
        "    # Attribute names are hardcoded into the files\n",
        "    entropy = torch.tensor(measures['uncertainty_measures']['cluster_assignment_entropy']).to(torch.float32)\n",
        "\n",
        "    accuracies = torch.tensor([record['most_likely_answer']['accuracy'] for record in generations.values()])\n",
        "\n",
        "    # hidden states for TBG (token before model generation)\n",
        "    tbg_dataset = torch.stack([record['most_likely_answer']['emb_last_tok_before_gen']\n",
        "                               for record in generations.values()]).squeeze(-2).transpose(0, 1).to(torch.float32)\n",
        "\n",
        "    # hidden states for SLT (second last token of model generation)\n",
        "    slt_dataset = torch.stack([record['most_likely_answer']['emb_tok_before_eos']\n",
        "                               for record in generations.values()]).squeeze(-2).transpose(0, 1).to(torch.float32)\n",
        "\n",
        "\n",
        "    return (tbg_dataset[:, :n_sample, :], slt_dataset[:, :n_sample, :], entropy[:n_sample], accuracies[:n_sample])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_paths"
      ],
      "metadata": {
        "id": "ojJQ89Bmm0rv",
        "outputId": "9cb185b2-5e4d-4fd2-f90d-156e2b973581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ojJQ89Bmm0rv",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wandb_runs/Llama2-7b/run-mb6th09c',\n",
              " 'wandb_runs/Llama2-7b/run-x84t13ca',\n",
              " 'wandb_runs/Llama2-7b/run-wxvq5m6a',\n",
              " 'wandb_runs/Llama2-7b/run-9qv44yea']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e83e3683-6b83-4a35-a49a-f0f218d26e74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "e83e3683-6b83-4a35-a49a-f0f218d26e74",
        "outputId": "a44aaf9e-0cb8-459c-9c81-4400ce8f2934"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'validation_generations.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6f42d73b6871>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mDs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Set essential attributes for prettier printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9db6374b3fb8>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, n_sample)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Read validation generated embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VAL_GEN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'validation_generations.pkl'"
          ]
        }
      ],
      "source": [
        "# Load all datasets\n",
        "Ds = []\n",
        "for path in ds_paths:\n",
        "    Ds.append(Dataset(load_dataset(os.path.join(notebook_path, path))))\n",
        "\n",
        "# Set essential attributes for prettier printing\n",
        "for i, D in enumerate(Ds):\n",
        "    D.name = ds_names[i]\n",
        "    D.path = ds_paths[i]\n",
        "    # OOD-related\n",
        "    D.other_ids = [j for j in range(len(Ds)) if j != i]\n",
        "    D.other_names = [ds_names[j] for j in D.other_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac7b9b95-9b0d-4938-ab97-c30ab39a125e",
      "metadata": {
        "id": "ac7b9b95-9b0d-4938-ab97-c30ab39a125e"
      },
      "source": [
        "## Probing Acc/SE from Hidden States"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "030caba9-c619-4b9a-b68d-a19f5c86eca1",
      "metadata": {
        "id": "030caba9-c619-4b9a-b68d-a19f5c86eca1"
      },
      "source": [
        "### Helper methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8da16b-dc3f-40a5-ab08-09783ab703fa",
      "metadata": {
        "id": "2f8da16b-dc3f-40a5-ab08-09783ab703fa"
      },
      "outputs": [],
      "source": [
        "# Create train/val/test splits\n",
        "def create_Xs_and_ys(datasets, scores, val_test_splits=[0.2, 0.1], test_only=False, no_val=False):\n",
        "    # Data splitting for sklearn linear models\n",
        "    X = np.array(datasets)\n",
        "    y = np.array(scores)\n",
        "\n",
        "    if test_only:\n",
        "        X_tests, y_tests = [], []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            X_tests.append(X[i])\n",
        "            y_tests.append(y)\n",
        "        return (None, None, X_tests, None, None, y_tests)\n",
        "\n",
        "    valid_size = val_test_splits[0]\n",
        "    test_size = val_test_splits[1]\n",
        "\n",
        "    X_trains, X_vals, X_tests, y_trains, y_vals, y_tests = [], [], [], [], [], []\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        # Split data into train, validation, and test sets\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X[i], y, test_size=test_size, random_state=42)\n",
        "        X_tests.append(X_test)\n",
        "        y_tests.append(y_test)\n",
        "        if no_val:\n",
        "            X_trains.append(X_train_val)\n",
        "            y_trains.append(y_train_val)\n",
        "            continue\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=valid_size, random_state=42)\n",
        "        X_trains.append(X_train)\n",
        "        y_trains.append(y_train)\n",
        "        X_vals.append(X_val)\n",
        "        y_vals.append(y_val)\n",
        "\n",
        "    return X_trains, X_vals, X_tests, y_trains, y_vals, y_tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295e3166-a4e0-4e3c-aad3-5b6523b7dd62",
      "metadata": {
        "id": "295e3166-a4e0-4e3c-aad3-5b6523b7dd62"
      },
      "outputs": [],
      "source": [
        "# Bootstrapping methods from ../semantic_entropy/uncertainty/utils/eval_utils.py\n",
        "def bootstrap_func(y_true, y_score, func):\n",
        "    y_tuple = (y_true, y_score)\n",
        "\n",
        "    metric_i = func(*y_tuple)\n",
        "    metric_dict = {}\n",
        "    metric_dict['mean'] = metric_i\n",
        "    metric_dict['bootstrap'] = compatible_bootstrap(\n",
        "        func, rng)(*y_tuple)  # a bit slow to run\n",
        "\n",
        "    return metric_dict\n",
        "\n",
        "def bootstrap(function, rng, n_resamples=1000):\n",
        "    def inner(data):\n",
        "        bs = scipy.stats.bootstrap(\n",
        "            (data, ), function, n_resamples=n_resamples, confidence_level=0.9,\n",
        "            random_state=rng)\n",
        "        return {\n",
        "            'std_err': bs.standard_error,\n",
        "            'low': bs.confidence_interval.low,\n",
        "            'high': bs.confidence_interval.high\n",
        "        }\n",
        "    return inner\n",
        "\n",
        "def auroc(y_true, y_score):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
        "    del thresholds\n",
        "    return metrics.auc(fpr, tpr)\n",
        "\n",
        "def compatible_bootstrap(func, rng):\n",
        "    def helper(y_true_y_score):\n",
        "        # this function is called in the bootstrap\n",
        "        y_true = np.array([i['y_true'] for i in y_true_y_score])\n",
        "        y_score = np.array([i['y_score'] for i in y_true_y_score])\n",
        "        out = func(y_true, y_score)\n",
        "        return out\n",
        "\n",
        "    def wrap_inputs(y_true, y_score):\n",
        "        return [{'y_true': i, 'y_score': j} for i, j in zip(y_true, y_score)]\n",
        "\n",
        "    def converted_func(y_true, y_score):\n",
        "        y_true_y_score = wrap_inputs(y_true, y_score)\n",
        "        return bootstrap(helper, rng=rng)(y_true_y_score)\n",
        "    return converted_func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17594a64-634a-4bb5-b737-ccc891a1a6d3",
      "metadata": {
        "id": "17594a64-634a-4bb5-b737-ccc891a1a6d3"
      },
      "outputs": [],
      "source": [
        "# Train and evaluation function.\n",
        "def sklearn_train_and_evaluate(model, X_train, y_train, X_valid, y_valid, silent=False):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate training loss and score\n",
        "    train_probs = model.predict_proba(X_train)\n",
        "    train_loss = log_loss(y_train, train_probs)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    valid_preds = model.predict(X_valid)\n",
        "    valid_probs = model.predict_proba(X_valid)\n",
        "    valid_loss = log_loss(y_valid, valid_probs)\n",
        "    val_accuracy = np.mean((valid_preds == y_valid).astype(int))\n",
        "    auroc_score = roc_auc_score(y_valid, valid_probs[:,1])\n",
        "    if not silent:\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}, AUROC: {auroc_score:.4f}\")\n",
        "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
        "\n",
        "def sklearn_evaluate_on_test(model, X_test, y_test, silent=False, bootstrap=True):\n",
        "    test_preds = model.predict(X_test)\n",
        "    test_probs = model.predict_proba(X_test)\n",
        "    test_loss = log_loss(y_test, test_probs)\n",
        "    test_accuracy = np.mean((test_preds == y_test).astype(int))\n",
        "\n",
        "    if bootstrap:\n",
        "        auroc_score = bootstrap_func(y_test, test_probs[:,1], auroc)\n",
        "        auroc_score_scalar = auroc_score['mean']\n",
        "    else:\n",
        "        auroc_score = auroc_score_scalar = roc_auc_score(y_test, test_probs[:, 1])\n",
        "\n",
        "    if not silent:\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, AUROC: {auroc_score_scalar:.4f}\")\n",
        "\n",
        "    return test_loss, test_accuracy, auroc_score\n",
        "\n",
        "def train_single_metric(D, token_type='tbg', metric='b_entropy'):\n",
        "    \"\"\"train and test on single metric (e.g. SE, Acc) on single dataset\"\"\"\n",
        "    var_name = token_type[0] + metric[0]\n",
        "    # named as [te, se. ta, sa] for easy identification; t for tbg, s for slt (token positions)\n",
        "    # e for entropy and a for accuracy (or model faithfulness)\n",
        "    X_trains, X_vals, X_tests, y_trains, y_vals, y_tests = create_Xs_and_ys(\n",
        "        getattr(D, f'{token_type}_dataset'), getattr(D, metric)\n",
        "    )\n",
        "\n",
        "    accs = []\n",
        "    aucs = []\n",
        "    models = []\n",
        "\n",
        "    for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(zip(X_trains, X_vals, X_tests, y_trains, y_vals, y_tests)):\n",
        "        print(f\"Training on {D.name}-{token_type.upper()}-{metric.upper()} {i+1}/{len(X_trains)}\")\n",
        "        model = LogisticRegression()\n",
        "        sklearn_train_and_evaluate(model, X_train, y_train, X_val, y_val)\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test, y_test)\n",
        "        accs.append(test_acc)\n",
        "        aucs.append(test_auc)\n",
        "        models.append(model)\n",
        "\n",
        "    setattr(D, f'{var_name}_accs', accs)\n",
        "    setattr(D, f'{var_name}_aucs', aucs)\n",
        "    setattr(D, f'{var_name}_models', models)\n",
        "\n",
        "# simple get-around for unpacking bootstrapping dicts\n",
        "auc = lambda aucs : [ac['mean'] for ac in aucs]\n",
        "idf = lambda x : x  # identical function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4308f46-b465-4f75-bdcd-3251cd7fefbd",
      "metadata": {
        "id": "c4308f46-b465-4f75-bdcd-3251cd7fefbd"
      },
      "outputs": [],
      "source": [
        "# Plotting methods\n",
        "def plot_metrics_ax(ax, test_metrics_list, model_names, title=\"\", prep_func=auc,\n",
        "                    use_logarithm=False, preset_layer_indices=None, legend_outside=False):  # some simple gadgets\n",
        "    \"\"\"plot metrics along certain axis in a multi-axis plot (plt.subplots)\"\"\"\n",
        "    if len(test_metrics_list) != len(model_names):\n",
        "        raise ValueError(\"The length of test_metrics_list and model_names must be the same.\")\n",
        "\n",
        "    for test_metrics, model_name in zip(test_metrics_list, model_names):\n",
        "        test_metrics = torch.tensor(prep_func(test_metrics), dtype=torch.float32)\n",
        "        if use_logarithm:\n",
        "            test_metrics = torch.log(test_metrics + 1e-6)\n",
        "        if preset_layer_indices is not None:\n",
        "            layer_indices = preset_layer_indices\n",
        "        else:\n",
        "            layer_indices = torch.arange(len(test_metrics)) + 1  # +1 if layer indexing starts at 1\n",
        "\n",
        "        ax.plot(layer_indices, test_metrics, marker='o', linestyle='-', linewidth=2, markersize=5, label=model_name)\n",
        "\n",
        "    ax.set_title(f'{title}', fontsize=14)\n",
        "    ax.set_xlabel('Layer', fontsize=12)\n",
        "    ax.set_ylabel(f'Test AUROC scores', fontsize=12)\n",
        "\n",
        "    tick_interval = 5  # Change this value to display more or fewer ticks\n",
        "    ax.set_xticks(layer_indices[::tick_interval].tolist())\n",
        "    ax.set_xticklabels(layer_indices[::tick_interval].tolist())\n",
        "\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    if legend_outside:\n",
        "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)\n",
        "    else:\n",
        "        ax.legend(fontsize=12)\n",
        "\n",
        "def save_fig(name):\n",
        "    \"\"\"save figure with timestamps\"\"\"\n",
        "    full_path = f'{notebook_path}/figures'\n",
        "    full_path = os.path.expanduser(full_path)\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "    plt.savefig(f'{full_path}/{name}.pdf', format='pdf', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98205177-eb13-4013-bb3e-a06bb07311d9",
      "metadata": {
        "id": "98205177-eb13-4013-bb3e-a06bb07311d9"
      },
      "outputs": [],
      "source": [
        "# Best split for SE binarization.\n",
        "def best_split(entropy: torch.Tensor, label=\"Dx\"):\n",
        "    \"\"\"\n",
        "    Identify best split for minimizing reconstruction error via low and high SE mean estimates,\n",
        "    as discussed in Section 4. Binarization of paper (ArXiv: 2406.15927)\n",
        "    \"\"\"\n",
        "    ents = entropy.numpy()\n",
        "    splits = np.linspace(1e-10, ents.max(), 100)\n",
        "    split_mses = []\n",
        "    for split in splits:\n",
        "        low_idxs, high_idxs = ents < split, ents >= split\n",
        "\n",
        "        low_mean = np.mean(ents[low_idxs])\n",
        "        high_mean = np.mean(ents[high_idxs])\n",
        "\n",
        "        mse = np.sum((ents[low_idxs] - low_mean)**2) + np.sum((ents[high_idxs] - high_mean)**2)\n",
        "        mse = np.sum(mse)\n",
        "\n",
        "        split_mses.append(mse)\n",
        "\n",
        "    split_mses = np.array(split_mses)\n",
        "\n",
        "    plt.plot(splits, split_mses, label=label)\n",
        "    return splits[np.argmin(split_mses)]\n",
        "\n",
        "def binarize_entropy(entropy, thres=0.0):  # 0.0 means even splits for normalized entropy scores\n",
        "    \"\"\"Binarize entropy scores into 0s and 1s\"\"\"\n",
        "    binary_entropy = torch.full_like(entropy, -1, dtype=torch.float)\n",
        "    binary_entropy[entropy < thres] = 0\n",
        "    binary_entropy[entropy > thres] = 1\n",
        "\n",
        "    return binary_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc4dc38-fc76-4bae-a1d8-cd2fc0a3c68a",
      "metadata": {
        "id": "5cc4dc38-fc76-4bae-a1d8-cd2fc0a3c68a"
      },
      "source": [
        "### Binarize Semantic Entropy (SE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd56c8ab-6d41-4dea-930b-daa29545ca0b",
      "metadata": {
        "id": "cd56c8ab-6d41-4dea-930b-daa29545ca0b"
      },
      "source": [
        "We use `best-universal-split` in our experiments (see more details on binarization in Section 4 of [our paper](https://arxiv.org/pdf/2406.15927))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf7ee42-863b-4f4d-9674-820202ea16ae",
      "metadata": {
        "id": "9cf7ee42-863b-4f4d-9674-820202ea16ae"
      },
      "outputs": [],
      "source": [
        "# Best universal split across datasets\n",
        "split_method = 'best-universal-split'\n",
        "all_entropy = torch.cat([D.entropy for D in Ds], dim=0)\n",
        "split = best_split(all_entropy, \"All datasets collective\")\n",
        "plt.legend()\n",
        "plt.title('Sum of squared errors at different splits')\n",
        "for D in Ds:\n",
        "    D.b_entropy = binarize_entropy(D.entropy, split)\n",
        "    print(f\"Dummy accuracy for {D.name}: {max(torch.mean(D.b_entropy).item(), 1-torch.mean(D.b_entropy).item()):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e78ef0-6914-4e71-b524-164648a450fa",
      "metadata": {
        "id": "f0e78ef0-6914-4e71-b524-164648a450fa"
      },
      "source": [
        "#### About trinarizing entropy:\n",
        "\n",
        "We can also leave out middle SE portion for better probing; we adopted this strategy for Llama2-70b long-form generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2bdd15d-dd90-408e-ba9e-230922ad23d4",
      "metadata": {
        "id": "d2bdd15d-dd90-408e-ba9e-230922ad23d4"
      },
      "outputs": [],
      "source": [
        "def trinarize_entropy(entropy, thres1=0.639, thres2=1.478):\n",
        "    \"\"\"Trinarize entropy scores into 0's and 1's\"\"\"\n",
        "    trinary_entropy = torch.full_like(entropy, -1, dtype=torch.float)\n",
        "    trinary_entropy[entropy < thres1] = 0\n",
        "    trinary_entropy[entropy > thres2] = 1\n",
        "    return trinary_entropy\n",
        "\n",
        "split_method = 'tri-universal-split'\n",
        "for D in Ds:\n",
        "    D.t_entropy = t_entropy = trinarize_entropy(D.entropy, split, split)\n",
        "    D.t_slt_dataset = D.slt_dataset[:, t_entropy != -1, :]\n",
        "    D.t_tbg_dataset = D.tbg_dataset[:, t_entropy != -1, :]\n",
        "    D.t_entropy = D.t_entropy[t_entropy != -1]\n",
        "    D.t_accuracies = D.accuracies[t_entropy != -1]\n",
        "    print('current left samples:', len(D.t_entropy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d430c354-fec9-4bd6-8c87-4b1762a8558f",
      "metadata": {
        "id": "d430c354-fec9-4bd6-8c87-4b1762a8558f"
      },
      "source": [
        "### Train linear probes to predict Binarized SE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b634a81-9caa-45d3-bbeb-3d4c66cc4825",
      "metadata": {
        "scrolled": true,
        "id": "9b634a81-9caa-45d3-bbeb-3d4c66cc4825"
      },
      "outputs": [],
      "source": [
        "for D in Ds:\n",
        "    train_single_metric(D, 'tbg', 'b_entropy')\n",
        "    train_single_metric(D, 'slt', 'b_entropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1ab5e9-0e8b-4586-886c-c858d27c7b35",
      "metadata": {
        "id": "ba1ab5e9-0e8b-4586-886c-c858d27c7b35"
      },
      "outputs": [],
      "source": [
        "# plot se probe\n",
        "fig, axs = plt.subplots(1, len(Ds), figsize=(5 * len(Ds), 4.5))\n",
        "for i, D in enumerate(Ds):\n",
        "    plot_metrics_ax(axs[i], [auc(D.tb_aucs), auc(D.sb_aucs)], [\"SE Probe on TBG token\", \"SE Probe on SLT token\"],\n",
        "                    f\"AUROC on {D.name.upper()}\", prep_func=idf)\n",
        "plt.tight_layout()\n",
        "save_fig(name=\"se_both_tok\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f38236-a602-453a-af19-ab3245b376d7",
      "metadata": {
        "id": "02f38236-a602-453a-af19-ab3245b376d7"
      },
      "source": [
        "### Train linear probes to predict Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a60df52-aa3d-403e-8fc8-4123423fbd05",
      "metadata": {
        "scrolled": true,
        "id": "0a60df52-aa3d-403e-8fc8-4123423fbd05"
      },
      "outputs": [],
      "source": [
        "for D in Ds:\n",
        "    train_single_metric(D, 'tbg', 'accuracies')\n",
        "    train_single_metric(D, 'slt', 'accuracies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e501c366-215e-46e2-b4c6-eb2a71bb4aeb",
      "metadata": {
        "id": "e501c366-215e-46e2-b4c6-eb2a71bb4aeb"
      },
      "outputs": [],
      "source": [
        "# plot acc probe\n",
        "fig, axs = plt.subplots(1, len(Ds), figsize=(5 * len(Ds), 4.5))\n",
        "for i, D in enumerate(Ds):\n",
        "    plot_metrics_ax(axs[i], [auc(D.ta_aucs), auc(D.sa_aucs)], [\"Acc Probe on TBG token\", \"Acc Probe on SLT token\"],\n",
        "                    f\"AUROC on {D.name.upper()}\", prep_func=idf)\n",
        "plt.tight_layout()\n",
        "save_fig(name=\"acc_both_tok\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418d3485-958c-47dd-94db-5e35c0ca4ea8",
      "metadata": {
        "id": "418d3485-958c-47dd-94db-5e35c0ca4ea8"
      },
      "source": [
        "### Use linear probe trained on Binarized SE to predict Accuracy\n",
        "\n",
        "We leverage trained probes on semantic entropy to predict model correctness on the same dataset.\n",
        "\n",
        "Note that we need to predict the error rate (1 minus accuracy) using SEPs due to the nature of semantic uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c0b379-5f6f-4abb-b625-e5d399eaf01f",
      "metadata": {
        "scrolled": true,
        "id": "41c0b379-5f6f-4abb-b625-e5d399eaf01f"
      },
      "outputs": [],
      "source": [
        "for D in Ds:\n",
        "    r_acc = 1 - D.accuracies\n",
        "\n",
        "    # TBG\n",
        "    _, _, X_tests, _, _, y_tests = create_Xs_and_ys(D.tbg_dataset, r_acc)\n",
        "    tab_accs = []\n",
        "    tab_aucs = []\n",
        "\n",
        "    for i, (X_test, y_test) in enumerate(zip(X_tests, y_tests)):\n",
        "        print(f\"Testing on {D.name}-TBG {i+1}/{len(X_tests)}\")\n",
        "        model = D.tb_models[i]\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test, y_test)\n",
        "        tab_accs.append(test_acc)\n",
        "        tab_aucs.append(test_auc)\n",
        "\n",
        "    D.tab_accs = tab_accs\n",
        "    D.tab_aucs = tab_aucs\n",
        "\n",
        "    # SLT t\n",
        "    _, _, X_tests, _, _, y_tests = create_Xs_and_ys(D.slt_dataset, r_acc)\n",
        "    sab_accs = []\n",
        "    sab_aucs = []\n",
        "\n",
        "    for i, (X_test, y_test) in enumerate(zip(X_tests, y_tests)):\n",
        "        print(f\"Testing on {D.name}-SLT {i+1}/{len(X_tests)}\")\n",
        "        model = D.sb_models[i]\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test, y_test)\n",
        "        sab_accs.append(test_acc)\n",
        "        sab_aucs.append(test_auc)\n",
        "\n",
        "    D.sab_accs = sab_accs\n",
        "    D.sab_aucs = sab_aucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "040c3ad8-fcb6-4c8f-830b-47d551d3ad7a",
      "metadata": {
        "id": "040c3ad8-fcb6-4c8f-830b-47d551d3ad7a"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, len(Ds), figsize=(5 * len(Ds), 4.5))\n",
        "for i, D in enumerate(Ds):\n",
        "    plot_metrics_ax(axs[i], [D.sa_aucs, D.sab_aucs], [\"LR trained and tested on Acc\", \"LR trained on SE, tested on Acc\"], f\"AUROC on {D.name.upper()}'s SLT\")\n",
        "plt.tight_layout()\n",
        "save_fig(name=\"se_for_acc_slt\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f064640d-619e-42c5-8b73-29d86c469662",
      "metadata": {
        "id": "f064640d-619e-42c5-8b73-29d86c469662"
      },
      "outputs": [],
      "source": [
        "# select best layer range (SLT)\n",
        "def decide_layer_range(Ds, metric='entropy', limit=33): # NOTE: set upperbound to be number of layers+1; e.g. for llama2-70b, it is 81.\n",
        "    \"\"\"\n",
        "    simple logic: use ID average test AUROCs across layers to determine\n",
        "    which consecutive range of layers did the best. Do separately\n",
        "    for SEP and Acc. Pr.\n",
        "    \"\"\"\n",
        "    assert hasattr(Ds[0], 'sab_aucs') and hasattr(Ds[0], 'sa_aucs'), 'previous cells need to be run'\n",
        "    if 'entropy' in metric:\n",
        "        aucs = [np.array(auc(D.sab_aucs)) for D in Ds]  # test metrics for ID SEPs\n",
        "    else:\n",
        "        aucs = [np.array(auc(D.sa_aucs)) for D in Ds]  # test metrics for ID APs\n",
        "    best_mean = -np.inf\n",
        "    best_range = []\n",
        "    average = lambda a,b : np.mean([np.mean(ac[a:b]) for ac in aucs])\n",
        "\n",
        "    for i in range(limit):\n",
        "        for j in range(i+1, limit):\n",
        "            if j - i < 5: # must be more than 5 layers\n",
        "                continue\n",
        "            if average(i, j) > best_mean:\n",
        "                best_mean = average(i, j)\n",
        "                best_range = [i, j]\n",
        "\n",
        "    return best_mean, best_range\n",
        "\n",
        "emean, (e1, e2) = decide_layer_range(Ds, 'entropy')\n",
        "amean, (a1, a2) = decide_layer_range(Ds, 'acc')\n",
        "for D in Ds:\n",
        "    D.sep_layer_range = (e1, e2)\n",
        "    D.ap_layer_range = (a1, a2)\n",
        "print(emean, (e1, e2), amean, (a1, a2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61e1fa3-5263-4369-a27d-7405c1e3840a",
      "metadata": {
        "id": "a61e1fa3-5263-4369-a27d-7405c1e3840a"
      },
      "outputs": [],
      "source": [
        "# BOOTSTRAP: select [xx-xx] layers for bootstrapping\n",
        "def concat_Xs_and_ys(layer_range, X_trains, X_vals, X_tests, y_trains, y_vals, y_tests,\n",
        "                     no_val=False, test_only=False):\n",
        "    \"\"\"\n",
        "    Concatenate @params{layer_range} hidden state layers on train/val/test sets.\n",
        "\n",
        "    no_val: no validation (training set only).\n",
        "    test_only: no train/validation set (test set only).\n",
        "    \"\"\"\n",
        "    if not no_val:\n",
        "        X_val_cc = np.concatenate(np.array(X_vals)[layer_range], axis=1)\n",
        "        y_val_cc = y_vals[layer_range[0]]\n",
        "    else:\n",
        "        X_val_cc = y_val_cc = None\n",
        "\n",
        "    if not test_only:\n",
        "        X_train_cc = np.concatenate(np.array(X_trains)[layer_range], axis=1)\n",
        "        y_train_cc = y_trains[layer_range[0]]\n",
        "    else:\n",
        "        X_train_cc = y_train_cc = None\n",
        "\n",
        "    X_test_cc = np.concatenate(np.array(X_tests)[layer_range], axis=1)\n",
        "    y_test_cc = y_tests[layer_range[0]]\n",
        "\n",
        "    return X_train_cc, X_val_cc, X_test_cc, y_train_cc, y_val_cc, y_test_cc\n",
        "\n",
        "def train_concat_SE(D, layer_range):\n",
        "    \"\"\"train model on single dataset SE with concatenated layers\"\"\"\n",
        "    for token_type in ['slt']: # optionally, ['slt', 'tbg']\n",
        "        var_name = token_type[0]\n",
        "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.b_entropy, test_only=True) # train on all data\n",
        "        _, _, X_train_cc, _, _, y_train_cc = concat_Xs_and_ys(layer_range, *all_Xs_and_ys, no_val=True, test_only=True)\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train_cc, y_train_cc)\n",
        "        setattr(D, f'{var_name}_bmodel', model)\n",
        "\n",
        "        print(f'{token_type.upper()} trained on {D.name} SE finished')\n",
        "\n",
        "def train_concat_Acc(D, layer_range):\n",
        "    \"\"\"train model on single dataset Accuracy with concatenated layers\"\"\"\n",
        "    for token_type in ['slt']: # optionally, ['slt', 'tbg']\n",
        "        var_name = token_type[0]\n",
        "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.accuracies, test_only=True) # train on all data\n",
        "        _, _, X_train_cc, _, _, y_train_cc = concat_Xs_and_ys(layer_range, *all_Xs_and_ys, no_val=True, test_only=True)\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train_cc, y_train_cc)\n",
        "        setattr(D, f'{var_name}_amodel', model)\n",
        "\n",
        "        print(f'{token_type.upper()} trained on {D.name} Acc finished')\n",
        "\n",
        "def train_concat_SE_Acc_test_Acc(D, layer_ranges=[list(range(e1,e2)), list(range(a1,a2))]):\n",
        "    \"\"\"ID: train and test SEPs and Acc. Pr. on single dataset with concatenated layers\"\"\"\n",
        "    for token_type in ['slt']: # optionally, ['slt', 'tbg']\n",
        "        var_name = token_type[0]\n",
        "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.b_entropy, no_val=True)\n",
        "\n",
        "        X_train_cc, _, _, y_train_cc, _, _ = concat_Xs_and_ys(layer_ranges[0], *all_Xs_and_ys, no_val=True)\n",
        "        ab_accs = []\n",
        "        ab_aucs = []\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train_cc, y_train_cc)\n",
        "\n",
        "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.accuracies)\n",
        "        _, _, X_test_cc, _, _, y_test_cc = concat_Xs_and_ys(layer_ranges[0], *all_Xs_and_ys) # fixed random seed ensures no data leakage\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test_cc, 1-y_test_cc) # SEP predicts error rate\n",
        "        ab_accs.append(test_acc)\n",
        "        ab_aucs.append(test_auc)\n",
        "\n",
        "        setattr(D, f'i{var_name}b_accs', ab_accs)  # i means IDß\n",
        "        setattr(D, f'i{var_name}b_aucs', ab_aucs)\n",
        "\n",
        "        print(f'{D.name.upper()}-{token_type.upper()} trainied on SE and tested on Acc finished')\n",
        "        aa_accs = []\n",
        "        aa_aucs = []\n",
        "        X_train_cc, _, X_test_cc, y_train_cc, _, y_test_cc = concat_Xs_and_ys(layer_ranges[1], *all_Xs_and_ys, no_val=True)\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train_cc, y_train_cc)\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test_cc, y_test_cc)\n",
        "        aa_accs.append(test_acc)\n",
        "        aa_aucs.append(test_auc)\n",
        "\n",
        "        setattr(D, f'i{var_name}a_accs', aa_accs)\n",
        "        setattr(D, f'i{var_name}a_aucs', aa_aucs)\n",
        "        print(f'{D.name.upper()}-{token_type.upper()} trainied on Acc and tested on Acc finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8a1906-4916-4a1a-8014-26f30247fe40",
      "metadata": {
        "id": "5e8a1906-4916-4a1a-8014-26f30247fe40"
      },
      "outputs": [],
      "source": [
        "# ID: train SEPs and Acc. Pr. and test them on Acc. from the same dataset\n",
        "# import warnings # uncomment to disable convergence warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "for D in Ds:\n",
        "    train_concat_SE_Acc_test_Acc(D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe24996-33f7-44a6-b0a0-e63abaa8d7df",
      "metadata": {
        "id": "0fe24996-33f7-44a6-b0a0-e63abaa8d7df"
      },
      "outputs": [],
      "source": [
        "# OOD: save model trained on individual datasets for OOD tests\n",
        "for D in Ds:\n",
        "    train_concat_SE(D, layer_range=list(range(e1,e2)))\n",
        "    train_concat_Acc(D, layer_range=list(range(a1,a2)))\n",
        "# outputs cleared for space concern"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1400993a-2316-4480-a095-2376c9d8a80a",
      "metadata": {
        "id": "1400993a-2316-4480-a095-2376c9d8a80a"
      },
      "source": [
        "## Test probes trained with one dataset on others\n",
        "\n",
        "We generalize SE and Acc. probes to Out-of-Distribution settings (OOD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c1bc9b-dab0-401f-8aa0-5f989ec7fd6d",
      "metadata": {
        "id": "f7c1bc9b-dab0-401f-8aa0-5f989ec7fd6d"
      },
      "outputs": [],
      "source": [
        "# Train on one's Acc/SE and test on others' Acc\n",
        "def test_one_on_n(D, token_type='slt',\n",
        "                  layer_range1=list(range(e1,e2)),\n",
        "                  layer_range2=list(range(a1,a2))):\n",
        "    var_name = token_type[0]\n",
        "    other_ids = D.other_ids\n",
        "    other_names = D.other_names\n",
        "    metric = 'accuracies'\n",
        "    a_model = getattr(D, f'{var_name}_amodel')  # Acc. Probe\n",
        "    b_model = getattr(D, f'{var_name}_bmodel')  # SE Probe\n",
        "\n",
        "    print(f\"Using probes trained on datasets {D.name.upper()}'s {token_type.upper()}-SE/Acc to predict {other_names}'s {token_type.upper()}-Acc\")\n",
        "\n",
        "    oa_accs = {}\n",
        "    oa_aucs = {}\n",
        "    ob_accs = {}\n",
        "    ob_aucs = {}\n",
        "\n",
        "    for id_ in D.other_ids:\n",
        "        D_id = Ds[id_]\n",
        "        print(f\"Testing on {D_id.name.upper()}'s {token_type.upper()}-{metric.upper()}\")\n",
        "        if metric == 'accuracies':\n",
        "            y_metric = 1 - getattr(D_id, metric)  # error rate\n",
        "        else:\n",
        "            y_metric = getattr(D_id, metric)\n",
        "\n",
        "        ida_accs = []\n",
        "        ida_aucs = []\n",
        "        idb_accs = []\n",
        "        idb_aucs = []\n",
        "\n",
        "        # create test sets with accuracy labels\n",
        "        all_Xs_and_ys = create_Xs_and_ys(getattr(D_id, f'{token_type}_dataset'), y_metric, test_only=True)\n",
        "\n",
        "        # test on Acc. Probes\n",
        "        _, _, X_test_cc, _, _, y_test_cc = concat_Xs_and_ys(layer_range2, *all_Xs_and_ys, no_val=True, test_only=True)\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(a_model, X_test_cc, 1-y_test_cc, bootstrap=True)\n",
        "        ida_accs.append(test_acc)\n",
        "        ida_aucs.append(test_auc)\n",
        "\n",
        "        # test on SE Probes\n",
        "        _, _, X_test_cc, _, _, y_test_cc = concat_Xs_and_ys(layer_range1, *all_Xs_and_ys, no_val=True, test_only=True)\n",
        "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(b_model, X_test_cc, y_test_cc, bootstrap=True)\n",
        "        idb_accs.append(test_acc)\n",
        "        idb_aucs.append(test_auc)\n",
        "\n",
        "        oa_accs[D_id.name] = ida_accs\n",
        "        oa_aucs[D_id.name] = ida_aucs\n",
        "        ob_accs[D_id.name] = idb_accs\n",
        "        ob_aucs[D_id.name] = idb_aucs\n",
        "\n",
        "    setattr(D, 'osa_accs', oa_accs) # o means OOD\n",
        "    setattr(D, 'osa_aucs', oa_aucs)\n",
        "    setattr(D, 'osb_accs', ob_accs)\n",
        "    setattr(D, 'osb_aucs', ob_aucs)\n",
        "\n",
        "    print(f\"Using probes trained on dataset {D.name.upper()} testing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b1a388-27c3-459c-ba65-165052b6caf5",
      "metadata": {
        "id": "e5b1a388-27c3-459c-ba65-165052b6caf5"
      },
      "outputs": [],
      "source": [
        "for D in Ds:\n",
        "    test_one_on_n(D, 'slt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daeaa10d-97f5-4565-ba82-6932aad7b7c2",
      "metadata": {
        "id": "daeaa10d-97f5-4565-ba82-6932aad7b7c2"
      },
      "outputs": [],
      "source": [
        "# compute the OOD average: mean([train on B-> test on A, train on C -> test on A, train on D-> test on A])\n",
        "b_performances = defaultdict(list)\n",
        "a_performances = defaultdict(list)\n",
        "win_rate = []\n",
        "for D in Ds:\n",
        "    for name in D.other_names:\n",
        "        b_performances[name].append(auc(D.osb_aucs[name]))\n",
        "        a_performances[name].append(auc(D.osa_aucs[name]))\n",
        "        if auc(D.osb_aucs[name]) > auc(D.osa_aucs[name]):\n",
        "            win_rate.append(1)\n",
        "        else:\n",
        "            win_rate.append(0)\n",
        "\n",
        "print(f\"winning rate: {np.mean(win_rate)*100:.2f}%\")\n",
        "\n",
        "for D in Ds:\n",
        "    D.sep_ood_avg = np.mean(b_performances[D.name])\n",
        "    D.ap_ood_avg = np.mean(a_performances[D.name])\n",
        "    print(f\"Average performance on {D.name}: SE Probe - {D.sep_ood_avg}, Acc Probe - {D.ap_ood_avg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c823c9be-2d2c-4f19-ba3f-6f7be9e40b42",
      "metadata": {
        "id": "c823c9be-2d2c-4f19-ba3f-6f7be9e40b42"
      },
      "source": [
        "## Baselines and Performance Plots\n",
        "\n",
        "Optional section for baselines and plotting barplots like in the paper.\n",
        "\n",
        "Baselines include log-likelihoods, p-true, naive (predictive) entropy, and semantic entropy predicted model correctness, in the form of AUROC score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ebea2f0-cae9-4ff1-b839-0bf13d4a61d7",
      "metadata": {
        "id": "6ebea2f0-cae9-4ff1-b839-0bf13d4a61d7"
      },
      "outputs": [],
      "source": [
        "# log likelihood baseline (a bit slow due to file loading)\n",
        "for D in Ds:\n",
        "    with open(f\"{D.path}/{run_files['VAL_GEN']}\", 'rb') as file:\n",
        "        gens = pickle.load(file)\n",
        "    liks = torch.tensor([np.mean(record['most_likely_answer']['token_log_likelihoods']) for record in gens.values()])\n",
        "    accs = torch.tensor([record['most_likely_answer']['accuracy'] for record in gens.values()])\n",
        "    probs = np.exp(liks)\n",
        "    D.loglik = bootstrap_func(accs, probs, auroc)\n",
        "    print(f\"{D.name} log-lik: {D.loglik['mean']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4293af54-e078-49db-b8b4-4e9a47de4546",
      "metadata": {
        "id": "4293af54-e078-49db-b8b4-4e9a47de4546"
      },
      "outputs": [],
      "source": [
        "# p_true and naive entropy baselines from uncertainty_measures and wandb_summary\n",
        "# You should've enabled `--compute_p_true` in generation runs.\n",
        "for i, path in enumerate(ds_paths):\n",
        "    os.chdir(path)\n",
        "\n",
        "    # no need to run again once collected\n",
        "    if hasattr(Ds[i], 'p_false_fixed') and hasattr(Ds[i], 'baseline_se'):\n",
        "        continue\n",
        "\n",
        "    with open(run_files['UNC_MEA'], 'rb') as file:\n",
        "        measures = pickle.load(file)\n",
        "\n",
        "    p_false_fixed_tuple = (measures['validation_is_false'], measures['uncertainty_measures']['p_false_fixed'])\n",
        "\n",
        "    metric_i = auroc(*p_false_fixed_tuple)\n",
        "    p_false_fixed_dict = {}\n",
        "    p_false_fixed_dict['mean'] = metric_i\n",
        "    p_false_fixed_dict['bootstrap'] = compatible_bootstrap(\n",
        "        auroc, rng)(*p_false_fixed_tuple)  # a bit slow to run\n",
        "\n",
        "    Ds[i].p_false_fixed = p_false_fixed_dict\n",
        "\n",
        "    with open(run_files['WAN_SUM'], 'rb') as file:  # auto-collected with analyze_run.py\n",
        "        boots = json.load(file)\n",
        "\n",
        "    Ds[i].baseline_se = boots['uncertainty']['cluster_assignment_entropy']['AUROC']\n",
        "    Ds[i].baseline_re = boots['uncertainty']['regular_entropy']['AUROC']\n",
        "\n",
        "    print(f\"Baselines for {Ds[i].name} computed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29068ff-9b9f-4e41-8ab9-b4d1ff7ad27c",
      "metadata": {
        "id": "a29068ff-9b9f-4e41-8ab9-b4d1ff7ad27c"
      },
      "outputs": [],
      "source": [
        "# plot performances\n",
        "def plot_performance_barplots(Ds, types=['ood', 'id']):\n",
        "    n = len(Ds)\n",
        "    for type_ in types:\n",
        "        fig, axes = plt.subplots(1, n, figsize=(n * 6, 6), sharey=True)\n",
        "\n",
        "        for i, (D, ax) in enumerate(zip(Ds, axes)):\n",
        "            x_labels = ['semantic entropy probe', 'accuracy probe', 'log likelihood', 'regular entropy', 'p_true', 'semantic entropy']\n",
        "\n",
        "            if type_ == 'id':\n",
        "                values = [D.isb_aucs[0]['mean'], D.isa_aucs[0]['mean'], D.loglik['mean'], D.baseline_re['mean'],\n",
        "                         0.5 if D.name == 'bioasq' else D.p_false_fixed['mean'], D.baseline_se['mean']]\n",
        "            elif type_ == 'ood':\n",
        "                values = [D.sep_ood_avg, D.ap_ood_avg, D.loglik['mean'], D.baseline_re['mean'],\n",
        "                          0.5 if D.name == 'bioasq' else D.p_false_fixed['mean'], D.baseline_se['mean']]\n",
        "            else:\n",
        "                return\n",
        "\n",
        "            bars = ax.bar(x_labels, values, width=0.4, capsize=5)\n",
        "            ax.set_xlabel('Methods')\n",
        "            if i == 0:\n",
        "                ax.set_ylabel('Test AUROC scores')\n",
        "            ax.set_ylim(0.2, 1.0)\n",
        "            ax.set_title(f\"{D.name.upper()}-SLT {type_.upper()}\")\n",
        "            ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
        "\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_fig(f'{type_}_performance_barplot')\n",
        "        plt.show()\n",
        "\n",
        "plot_performance_barplots(Ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925305cf-bcb1-42b1-b003-116cb1eebc3e",
      "metadata": {
        "id": "925305cf-bcb1-42b1-b003-116cb1eebc3e"
      },
      "source": [
        "## Save trained probes for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2758513-e2d4-47a5-b74e-1f6a6d7c3a67",
      "metadata": {
        "id": "b2758513-e2d4-47a5-b74e-1f6a6d7c3a67"
      },
      "outputs": [],
      "source": [
        "# save models to pickle for inference\n",
        "Ds_save = ()\n",
        "to_save_attrs = ['s_amodel', 's_bmodel', 't_amodel', 't_bmodel', 'sep_layer_range', 'ap_layer_range', 'name']\n",
        "\n",
        "for D in Ds:\n",
        "    D_save = {key: value for key, value in vars(D).items() if key in to_save_attrs}\n",
        "    Ds_save += (D_save,)\n",
        "\n",
        "path = f\"{notebook_path}/models\"\n",
        "full_path = os.path.expanduser(path)\n",
        "os.makedirs(full_path, exist_ok=True)\n",
        "\n",
        "with open(f'{full_path}/{model_name}_inference.pkl', 'wb') as f:\n",
        "    pickle.dump(Ds_save, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}